{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the notebook, I will write a code from beginning vs cifar10\n",
    "\n",
    "https://www.kaggle.com/c/cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed package\n",
    "import sys\n",
    "sys.path.insert(0,'../../')\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from mxnet import gluon, init, autograd, nd\n",
    "from mxnet.gluon import data as gdata, nn, loss as gloss, utils as gutils\n",
    "import datetime\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First read the file, all the files are zip in the ../../kaggle_cifar10, need first unzip them\n",
    "demo = False\n",
    "\n",
    "if demo == True:\n",
    "    import zipfile\n",
    "    for f in [\"train_tiny.zip\",\"test_tiny.zip\",\"trainLabels.csv.zip\"]:\n",
    "        with zipfile.ZipFile(\"../../data/kaggle_cifar10/\"+f,'r') as z:\n",
    "                             z.extractall(\"../../data/kaggle_cifar10/\")\n",
    "#for train.7z, use the \"7z x train.7z\" to unzip in the shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we need to reorg the data\n",
    "def reorg_data(data_dir, label_file, train_dir,test_dir,input_dir,valid_ratio):\n",
    "    \n",
    "    #read label id:label in the file\n",
    "    with open(os.path.join(data_dir,label_file)) as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        #print(lines)\n",
    "        #without rstrip, there will be \\n, for example ['1', 'frog\\n']\n",
    "        tokens = [l.rstrip().split(\",\") for l in lines]\n",
    "        #print(tokens)\n",
    "        #idx_label= dict(((int(idx), label) for idx, label in tokens))\n",
    "        idx_label= dict((int(idx), label) for idx, label in tokens)\n",
    "\n",
    "        #print(idx_label)\n",
    "    labels = set(idx_label.values())\n",
    "    print(labels)\n",
    "    \n",
    "    # got the train data number\n",
    "    n_train_all = len(os.listdir(os.path.join(data_dir,train_dir)))\n",
    "    n_train = n_train_all*(1-valid_ratio)\n",
    "    assert 0< n_train < n_train_all\n",
    "    n_train_per_label = n_train // len(labels)\n",
    "    print(n_train_per_label)\n",
    "    \n",
    "    def mkdir_if_no_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "   \n",
    "    #next we need to put the files into different folders with the folder name as the label\n",
    "    #track the file number copid to the folder\n",
    "    label_count={}\n",
    "    \n",
    "    #copy file process\n",
    "    for file in os.listdir(os.path.join(data_dir,train_dir)):\n",
    "        idx = int(file.split(\".\")[0])\n",
    "        label = idx_label[idx]\n",
    "        \n",
    "        if label not in label_count or label_count[label] < n_train_per_label:\n",
    "            \n",
    "            # put the label into dict or add 1 if already exist, make sure we can the number of train samples\n",
    "            label_count[label]=label_count.get(label,0)+1\n",
    "            #mkdir for the label \n",
    "            mkdir_if_no_exist([data_dir,input_dir,\"train\",label])\n",
    "            shutil.copy(os.path.join(data_dir,train_dir,file), os.path.join(data_dir,input_dir,\"train\",label))\n",
    "        else:\n",
    "            mkdir_if_no_exist([data_dir,input_dir,\"valid\",label])\n",
    "            shutil.copy(os.path.join(data_dir,train_dir,file), os.path.join(data_dir,input_dir,\"valid\",label))\n",
    "    \n",
    "    for file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        idx = int(file.split(\".\")[0])\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_no_exist([data_dir,input_dir,\"test\",\"unknown\"])\n",
    "        shutil.copy(os.path.join(data_dir,train_dir,file), os.path.join(data_dir,input_dir,\"test\",\"unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deer', 'frog', 'truck', 'dog', 'bird', 'horse', 'cat', 'automobile', 'ship', 'airplane'}\n",
      "4500.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "267068",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-fab65b5758eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvalid_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m reorg_data(data_dir, label_file, train_dir, test_dir, input_dir,\n\u001b[0;32m---> 11\u001b[0;31m                    valid_ratio)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-103-d401e6c2c1aa>\u001b[0m in \u001b[0;36mreorg_data\u001b[0;34m(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mmkdir_if_no_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 267068"
     ]
    }
   ],
   "source": [
    "if demo:\n",
    "    # 注意：此处使用小训练集和小测试集并将批量大小相应设小。\n",
    "    # 使用 Kaggle 比赛的完整数据集时可设批量大小为较大整数。\n",
    "    train_dir, test_dir, input_dir = 'train_tiny', 'test_tiny', \"tiny_dataset\"\n",
    "else:\n",
    "    train_dir, test_dir, input_dir = 'train', 'test', \"final_dataset\"\n",
    "\n",
    "data_dir, label_file = '../../data/kaggle_cifar10', 'trainLabels.csv'\n",
    "valid_ratio = 0.1\n",
    "reorg_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image transformation, resize, crop, flip and nomorlize\n",
    "transform_train = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.Resize(40),\n",
    "    gdata.vision.transforms.RandomResizedCrop(32, scale=(0.64,1), ratio = (1.0,1.0)),\n",
    "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
    "    #for ToTensor, check this: https://pytorch.org/docs/0.2.0/_modules/torchvision/transforms.html#ToTensor\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    gdata.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                      [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# for test, just normalize\n",
    "\n",
    "transform_test = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    gdata.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                      [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.vision.datasets.ImageFolderDataset\n",
    "train_ds = gdata.vision.ImageFolderDataset(os.path.join(data_dir, input_dir, \"train\"), flag=1)\n",
    "valid_ds = gdata.vision.ImageFolderDataset(os.path.join(data_dir, input_dir, \"valid\"), flag=1)\n",
    "#test_ds = gdata.vision.ImageFolderDataset(os.path.join(data_dir, input_dir, \"test\"), flag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def the resnet-18 network\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, num_channels, use_1x1conv = False, strides =1, **kargs):\n",
    "        \n",
    "        #for the supper, need further check why need to put the Resiual and self in the parameters\n",
    "        super(Residual, self).__init__(**kargs)\n",
    "        \n",
    "        # conv2 will not use the strides which will keep the shape I think\n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=strides)\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "        \n",
    "    def hybrid_forward(self, F, X):\n",
    "        #print(\"X:\"+str(X.shape))\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        #print(\"Y:\"+str(Y.shape))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        \n",
    "        # please note, if I use \"if use_1x1conv\", I will get NameError: name 'use_1x1conv' is not defined\n",
    "        #if use_1x1conv:\n",
    "        if self.conv3:\n",
    "            X=self.conv3(X)\n",
    "        # if 1x1 conv, the channel size maybe different and then there should be broadcast for the channel level.\n",
    "        # for the broad cast, can not broadcast from 1 to n, not 2 to n, for example:\n",
    "        #Check failed: l == 1 || r == 1 operands could not be broadcast together with shapes [4,6,6,6] [4,2,6,6]\n",
    "        # But it will work if [4,6,6,6] plus [4,1,6,6]<- broadcat this\n",
    "        return F.relu(Y+X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape for x and y: (1, 3, 32, 32) (1,)\n",
    "#for x, y in train_data:\n",
    "#    print(x.shape, y.shape)\n",
    "#    print(y.astype('float32'))\n",
    "#    blk = nn.HybridSequential()\n",
    "#!!!there will be error if here I put the channel to 16, because the x has 3 channel and can not be broad cast to 16\n",
    "#    blk.add(Residual(16))\n",
    "#    blk.initialize()\n",
    "    #blk.hybridize()\n",
    "#    y=blk(x)\n",
    "# 1. I got \"module 'mxnet.symbol' has no attribute 'Relu'\" error -- should be relu\n",
    "# 2. How the x (1, 3, 32, 32) can be broadcast to more channels such as (1,64,x,x)? - answer: In resnet18, the first\n",
    "# layter is conv2d with 64 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_18(number_classes):\n",
    "    net = nn.HybridSequential()\n",
    "    \n",
    "    # first layer, 64 channel with batch normal and relu activate. One strange thing is there should be maxpool, \n",
    "    #nn.MaxPool2D(pool_size=3, strides=2, padding=1), but no here\n",
    "    net.add(nn.Conv2D(64, kernel_size=3, padding=1, strides=1), nn.BatchNorm(), nn.Activation('relu'))\n",
    "    \n",
    "    def resnet_block(num_channels, resnet_size, first_block=False):\n",
    "        blk = nn.HybridSequential()\n",
    "        for f in range(resnet_size):\n",
    "            \n",
    "            #there will be 4 residula block, each has 2 residual object. The first residual object will not half\n",
    "            # the width and height, other block will half the w and h by strides 2.\n",
    "            if f == 0 and not first_block:\n",
    "                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.add(Residual(num_channels))\n",
    "        return blk\n",
    "    \n",
    "    #add 4 residual net object, double the channel number and half the W and H, the first object should not do the \n",
    "    #half W and H before in the previous one there should be a maxpool with strides 2, however, in this project, there\n",
    "    # is no max pool, so I think we should change the W and H, need to verfiy further?????? If no 1x1conv, the  Y+X,\n",
    "    # X will be broadcast\n",
    "    net.add(resnet_block(64, 2, first_block=True))\n",
    "    net.add(resnet_block(128, 2))\n",
    "    net.add(resnet_block(256, 2))\n",
    "    net.add(resnet_block(512, 2))\n",
    "    \n",
    "    #last add last dense layer last\n",
    "    net.add(nn.GlobalAvgPool2D(), nn.Dense(number_classes))\n",
    "    \n",
    "    # 1 conv + 4 * 4 + 1 last dense = 18, thats how resnet18 come from\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and initialize the net\n",
    "def get_net(ctx):\n",
    "    num_classes = 10\n",
    "    net = resnet_18(num_classes)\n",
    "    # initialize the net with context and init from mxnet\n",
    "    net.initialize(ctx=ctx, init = init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predict result which equal to y, and then mean the value in one batch\n",
    "def train_accuracy(y_hat, y):\n",
    "    #print(y_hat.shape)\n",
    "    #print(\"y.shape\"+str(y.shape))\n",
    "    # In the example, its the mean(), I am not sure why, should be sum()\n",
    "    # I was wrong, should be mean(), the train_data len is the iterate times, for example, if 90 samples and 5 batch\n",
    "    #size, the len(train_data) should be 18 instead of 90\n",
    "    return (y_hat.argmax(axis=1)==y.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch(batch, ctx):\n",
    "    features, labels = batch\n",
    "    #print(type(features))\n",
    "    # guarantee the labels and features have the same time\n",
    "    if (features.dtype != labels.dtype):\n",
    "        labels = labels.astype(features.dtype)\n",
    "    # copy all the data into ctx\n",
    "    #Splits an NDArray into len(ctx_list) slices along batch_axis and loads each slice to one context in ctx_list.\n",
    "    # please note, the split_and_load will return list of ndarray, so to use the list, use zip\n",
    "    return (gutils.split_and_load(features, ctx), gutils.split_and_load(labels,ctx), features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \n",
    "    #data_iter type: mxnet.gluon.data.dataloader.DataLoader\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx=[ctx]\n",
    "    acc = nd.array([0])\n",
    "    n = 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        \n",
    "        #features and labels are list type\n",
    "        #use zip to convert to ndarray\n",
    "        # features type<class 'list'>\n",
    "        #print(\"features type\" + str(type(features)))\n",
    "        #print(labels)\n",
    "        \n",
    "        #!!!!!!!!!features and labels are list of ndarray, need zip to iterate the value\n",
    "        for X,y in zip(features, labels):\n",
    "        # in the original example, its sum the result, however, in the train_acc, its mean(), why????? should be sum\n",
    "        #even for the train, right?\n",
    "        # Why need to copy the cpu? Maybe because the calculation on cpu will be faster?\n",
    "            # the type for X and y is <class 'mxnet.ndarray.ndarray.NDArray'>\n",
    "            acc += (y.astype('float32') == net(X).argmax(axis=1)).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "    return acc.asscalar()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the loss function\n",
    "loss=gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the train function\n",
    "# lr_decay will change the lr, lr_period decide the period to change the lr\n",
    "def train(net, train_data, valid_data, lr, wd, epochs, lr_period, lr_decay,ctx):\n",
    "    \n",
    "    # Need to check the details for sgd, momentum defined how many steps to use for the weigth change, wd weight decay\n",
    "    # for the normalization for the loss\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr, 'momentum':0.9, 'wd':wd})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    print(prev_time)\n",
    "    for epoch in range(epochs):\n",
    "        #update the learning rate periodly\n",
    "        if epoch > 0 and epoch%lr_period==0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        \n",
    "        train_l = 0\n",
    "        train_acc = 0\n",
    "        loop_number = 0\n",
    "        \n",
    "        for X, y in train_data:\n",
    "            #loop_number += 1\n",
    "            #print(\"loop number %d\"%loop_number)\n",
    "            # in one for loop, the batch_size examples will be processed\n",
    "            y = y.astype('float32').as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X.as_in_context(ctx))\n",
    "                #print(y_hat)\n",
    "                # this loss is very interesting, the net will output 10 output, and then SoftmaxCrossEntropyLoss will\n",
    "                #calculate the loss based on the y which like a index???\n",
    "                l=loss(y_hat,y)\n",
    "            l.backward()\n",
    "            # apply the gredient based on the batch_size\n",
    "            #https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.Trainer.step.html\n",
    "            trainer.step(batch_size)\n",
    "            # in one epoch, all the loss will be sum together into the train_l, will be divided by train sample number\n",
    "            \n",
    "            #print(l)\n",
    "            train_l += l.mean().asscalar()\n",
    "            \n",
    "            # sum the whole acc, will be divided by train sample number\n",
    "            train_acc += train_accuracy(y_hat,y)\n",
    "        #The divmod() method takes two numbers and returns a pair of numbers (a tuple) consisting of their \n",
    "        #quotient and remainder.\n",
    "        current_time = datetime.datetime.now()\n",
    "        h,reminder = divmod((current_time-prev_time).seconds, 3600)\n",
    "        m,s = divmod(reminder,60)\n",
    "        #time taken time: 0:1:37 if \"time: %d:%d:%d\n",
    "        #time taken time: 00:01:37 if \"time %02d:02d:02d\"\n",
    "        time_s = \"time: %02d:%02d:%02d\" % (h,m,s)\n",
    "        \n",
    "        if epoch%1 == 0:\n",
    "            if valid_data is not None:\n",
    "                valid_acc = evaluate_accuracy(valid_data, net, ctx)\n",
    "                print(\"epoch %d, time taken %s, train loss %f, train_acc %f, valid_cc %f, learning_rate %f\" \n",
    "                      %(epoch, time_s, train_l/len(train_data), train_acc/len(train_data), valid_acc, trainer.learning_rate))\n",
    "            else:\n",
    "                print(\"epoch %d, time taken %s, train loss %f, train_acc %f, learning_rate %f\" \n",
    "                      %(epoch, time_s, train_l/len(train_data),train_acc/len(train_data), trainer.learning_rate))\n",
    "        prev_time = current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the train data, with the loader, the label will be the 0,1,2 which will be used to mark the label\n",
    "batch_size = 256\n",
    "train_data = gdata.DataLoader(train_ds.transform_first(transform_train),batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = gdata.DataLoader(valid_ds.transform_first(transform_train),batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = gdata.DataLoader(test_ds.transform_first(transform_test),batch_size, shuffle=True, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 06:06:40.383812\n",
      "epoch 0, time taken time: 0:1:37, train loss 1.482020, train_acc 0.459329, valid_cc 0.499600, learning_rate 0.010000\n",
      "epoch 1, time taken time: 0:1:40, train loss 1.069495, train_acc 0.617488, valid_cc 0.598200, learning_rate 0.010000\n",
      "epoch 2, time taken time: 0:1:37, train loss 0.858871, train_acc 0.695387, valid_cc 0.686600, learning_rate 0.010000\n",
      "epoch 3, time taken time: 0:1:38, train loss 0.726189, train_acc 0.743002, valid_cc 0.676200, learning_rate 0.010000\n",
      "epoch 4, time taken time: 0:1:37, train loss 0.635439, train_acc 0.777123, valid_cc 0.725000, learning_rate 0.010000\n",
      "epoch 5, time taken time: 0:1:37, train loss 0.566913, train_acc 0.801978, valid_cc 0.763600, learning_rate 0.010000\n",
      "epoch 6, time taken time: 0:1:37, train loss 0.510294, train_acc 0.821091, valid_cc 0.755600, learning_rate 0.010000\n",
      "epoch 7, time taken time: 0:1:38, train loss 0.471041, train_acc 0.834651, valid_cc 0.778000, learning_rate 0.010000\n",
      "epoch 8, time taken time: 0:1:38, train loss 0.434613, train_acc 0.849002, valid_cc 0.797600, learning_rate 0.010000\n",
      "epoch 9, time taken time: 0:1:36, train loss 0.391781, train_acc 0.862768, valid_cc 0.818400, learning_rate 0.010000\n",
      "epoch 10, time taken time: 0:1:37, train loss 0.363032, train_acc 0.874414, valid_cc 0.818000, learning_rate 0.010000\n",
      "epoch 11, time taken time: 0:1:37, train loss 0.338159, train_acc 0.881385, valid_cc 0.836200, learning_rate 0.010000\n",
      "epoch 12, time taken time: 0:1:37, train loss 0.319215, train_acc 0.889023, valid_cc 0.822000, learning_rate 0.010000\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = try_gpu(), 100, 0.01, 5e-4, \n",
    "lr_period, lr_decay, net = 80, 0.1, get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data,  lr, wd, num_epochs, lr_period,\n",
    "      lr_decay, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
