{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the mxnet to do the test\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "Id                                                               \n",
       "1        2596      51      3                               258   \n",
       "2        2590      56      2                               212   \n",
       "3        2804     139      9                               268   \n",
       "4        2785     155     18                               242   \n",
       "5        2595      45      2                               153   \n",
       "\n",
       "    Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "Id                                                                    \n",
       "1                                0                              510   \n",
       "2                               -6                              390   \n",
       "3                               65                             3180   \n",
       "4                              118                             3090   \n",
       "5                               -1                              391   \n",
       "\n",
       "    Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "Id                                                 \n",
       "1             221             232            148   \n",
       "2             220             235            151   \n",
       "3             234             238            135   \n",
       "4             238             238            122   \n",
       "5             220             234            150   \n",
       "\n",
       "    Horizontal_Distance_To_Fire_Points     ...      Soil_Type32  Soil_Type33  \\\n",
       "Id                                         ...                                 \n",
       "1                                 6279     ...                0            0   \n",
       "2                                 6225     ...                0            0   \n",
       "3                                 6121     ...                0            0   \n",
       "4                                 6211     ...                0            0   \n",
       "5                                 6172     ...                0            0   \n",
       "\n",
       "    Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
       "Id                                                                    \n",
       "1             0            0            0            0            0   \n",
       "2             0            0            0            0            0   \n",
       "3             0            0            0            0            0   \n",
       "4             0            0            0            0            0   \n",
       "5             0            0            0            0            0   \n",
       "\n",
       "    Soil_Type39  Soil_Type40  Cover_Type  \n",
       "Id                                        \n",
       "1             0            0           5  \n",
       "2             0            0           5  \n",
       "3             0            0           2  \n",
       "4             0            0           2  \n",
       "5             0            0           5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the files should be the same, copy from sklearn project\n",
    "train_path='/home/ubuntu/learn-with-other-kaggle/data/train.csv'\n",
    "test_path='/home/ubuntu/learn-with-other-kaggle/data/test.csv'\n",
    "\n",
    "train_data=pd.read_csv(train_path, index_col = 'Id')\n",
    "test_data=pd.read_csv(test_path, index_col = 'Id')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "data = train_data.values\n",
    "np.random.shuffle(data)\n",
    "columns_list = train_data.columns\n",
    "\n",
    "shuffled_data = pd.DataFrame(data, columns=columns_list)\n",
    "\n",
    "\n",
    "features_train = shuffled_data.drop('Cover_Type', axis=1)\n",
    "label_train = shuffled_data['Cover_Type']\n",
    "label_train = label_train.apply(lambda x:x-1)\n",
    "#print(label_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(label_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd, nd, gluon, init\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_rate = 0.1\n",
    "valid_index = int(train_data.shape[0]*(1-valid_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert the pd dataframe to ndarray will be used in mxnet\n",
    "\n",
    "data_train_nd = nd.array(features_train.values[0:valid_index])\n",
    "label_train_nd = nd.array(label_train.values[0:valid_index])\n",
    "\n",
    "data_valid_nd = nd.array(features_train.values[valid_index:])\n",
    "label_valid_nd = nd.array(label_train.values[valid_index:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13608, 54)\n",
      "(1512, 54)\n"
     ]
    }
   ],
   "source": [
    "print(data_train_nd.shape)\n",
    "print(data_valid_nd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(ctx):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(100, activation='relu'))\n",
    "   # net.add(nn.Dense(30, activation='relu'))\n",
    "    net.add(nn.Dense(7))\n",
    "    net.initialize(ctx=ctx,init=init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,epochs, batch_size, lr, lr_period, lr_decay, train_data, label_data, wd, ctx,valid_data=None, valid_label=None):\n",
    "    data_iter = gdata.DataLoader(gdata.ArrayDataset(train_data,label_data), batch_size, shuffle=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate':lr,'wd':wd})\n",
    "    for epoch in range(0,epochs):\n",
    "        train_loss = 0\n",
    "        for X,y in data_iter:\n",
    "            with autograd.record():\n",
    "                y_hat=net(X.as_in_context(ctx))\n",
    "                l = loss(y_hat, y.as_in_context(ctx))\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += l.mean().asscalar()\n",
    "        if epoch > 1 and epoch % lr_period ==0:\n",
    "            #lr = trainer.learning_rate*lr_decay if trainer.learning_rate*lr_decay >0.00001 else 0.00001\n",
    "            lr = trainer.learning_rate*lr_decay\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "        if valid_data is not None:\n",
    "            valid_loss = loss(net(valid_data.as_in_context(ctx)),valid_label.as_in_context(ctx)).mean().asscalar()\n",
    "            if epoch%10 == 0:\n",
    "                print(\"epoch %d, Train loss %f, learning rate %f, valid_error %f\"%(epoch, train_loss/len(data_iter), \n",
    "                                                                               trainer.learning_rate, valid_loss))\n",
    "        else:\n",
    "            if epoch%10 == 0:\n",
    "                print(\"epoch %d, Train loss %f, learning rate %f\"%(epoch, train_loss/len(data_iter), \n",
    "                                                                               trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, Train loss 30.276760, learning rate 0.005000, valid_error 29.319826\n",
      "epoch 10, Train loss 2.970519, learning rate 0.005000, valid_error 2.766277\n",
      "epoch 20, Train loss 0.929497, learning rate 0.005000, valid_error 0.887433\n",
      "epoch 30, Train loss 0.912105, learning rate 0.005000, valid_error 0.985022\n",
      "epoch 40, Train loss 0.956712, learning rate 0.005000, valid_error 0.926211\n",
      "epoch 50, Train loss 0.998421, learning rate 0.005000, valid_error 0.970056\n",
      "epoch 60, Train loss 1.055178, learning rate 0.002500, valid_error 1.022920\n",
      "epoch 70, Train loss 0.990329, learning rate 0.002500, valid_error 0.964160\n",
      "epoch 80, Train loss 0.972052, learning rate 0.002500, valid_error 0.988213\n",
      "epoch 90, Train loss 0.976490, learning rate 0.002500, valid_error 1.054268\n",
      "epoch 100, Train loss 0.961436, learning rate 0.002500, valid_error 0.996256\n",
      "epoch 110, Train loss 0.950506, learning rate 0.002500, valid_error 0.930724\n",
      "epoch 120, Train loss 0.956749, learning rate 0.001250, valid_error 0.954169\n",
      "epoch 130, Train loss 0.920142, learning rate 0.001250, valid_error 0.922211\n",
      "epoch 140, Train loss 0.920832, learning rate 0.001250, valid_error 0.957233\n",
      "epoch 150, Train loss 0.918926, learning rate 0.001250, valid_error 0.915266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\", line 51, in __del__\n",
      "    check_call(_LIB.MXNDArrayFree(self.handle))\n",
      "AttributeError: handle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 160, Train loss 0.906649, learning rate 0.001250, valid_error 0.927737\n",
      "epoch 170, Train loss 0.913813, learning rate 0.001250, valid_error 0.891523\n",
      "epoch 180, Train loss 0.899743, learning rate 0.000625, valid_error 0.894926\n",
      "epoch 190, Train loss 0.885808, learning rate 0.000625, valid_error 0.890265\n",
      "epoch 200, Train loss 0.881673, learning rate 0.000625, valid_error 0.897281\n",
      "epoch 210, Train loss 0.878856, learning rate 0.000625, valid_error 0.873423\n",
      "epoch 220, Train loss 0.888113, learning rate 0.000625, valid_error 0.895761\n",
      "epoch 230, Train loss 0.879319, learning rate 0.000625, valid_error 0.893391\n",
      "epoch 240, Train loss 0.882475, learning rate 0.000313, valid_error 0.879950\n",
      "epoch 250, Train loss 0.870654, learning rate 0.000313, valid_error 0.873726\n",
      "epoch 260, Train loss 0.869188, learning rate 0.000313, valid_error 0.872551\n",
      "epoch 270, Train loss 0.859370, learning rate 0.000313, valid_error 0.859524\n",
      "epoch 280, Train loss 0.855381, learning rate 0.000313, valid_error 0.859365\n",
      "epoch 290, Train loss 0.811670, learning rate 0.000313, valid_error 0.810549\n",
      "epoch 300, Train loss 0.785912, learning rate 0.000156, valid_error 0.800233\n",
      "epoch 310, Train loss 0.777915, learning rate 0.000156, valid_error 0.779247\n",
      "epoch 320, Train loss 0.774733, learning rate 0.000156, valid_error 0.777628\n",
      "epoch 330, Train loss 0.766466, learning rate 0.000156, valid_error 0.766361\n",
      "epoch 340, Train loss 0.757003, learning rate 0.000156, valid_error 0.753336\n",
      "epoch 350, Train loss 0.753015, learning rate 0.000156, valid_error 0.749905\n",
      "epoch 360, Train loss 0.750997, learning rate 0.000078, valid_error 0.744100\n",
      "epoch 370, Train loss 0.742244, learning rate 0.000078, valid_error 0.731962\n",
      "epoch 380, Train loss 0.737152, learning rate 0.000078, valid_error 0.723365\n",
      "epoch 390, Train loss 0.734191, learning rate 0.000078, valid_error 0.719522\n",
      "epoch 400, Train loss 0.731679, learning rate 0.000078, valid_error 0.715054\n",
      "epoch 410, Train loss 0.729965, learning rate 0.000078, valid_error 0.715585\n",
      "epoch 420, Train loss 0.727686, learning rate 0.000039, valid_error 0.717037\n",
      "epoch 430, Train loss 0.721615, learning rate 0.000039, valid_error 0.706948\n",
      "epoch 440, Train loss 0.721768, learning rate 0.000039, valid_error 0.708160\n",
      "epoch 450, Train loss 0.718286, learning rate 0.000039, valid_error 0.709513\n",
      "epoch 460, Train loss 0.716692, learning rate 0.000039, valid_error 0.701086\n",
      "epoch 470, Train loss 0.709846, learning rate 0.000039, valid_error 0.696594\n",
      "epoch 480, Train loss 0.704714, learning rate 0.000020, valid_error 0.696527\n",
      "epoch 490, Train loss 0.699859, learning rate 0.000020, valid_error 0.689904\n",
      "epoch 500, Train loss 0.697523, learning rate 0.000020, valid_error 0.690812\n",
      "epoch 510, Train loss 0.694181, learning rate 0.000020, valid_error 0.689766\n",
      "epoch 520, Train loss 0.694782, learning rate 0.000020, valid_error 0.685867\n",
      "epoch 530, Train loss 0.693976, learning rate 0.000020, valid_error 0.684742\n",
      "epoch 540, Train loss 0.690522, learning rate 0.000010, valid_error 0.683659\n",
      "epoch 550, Train loss 0.688710, learning rate 0.000010, valid_error 0.681244\n",
      "epoch 560, Train loss 0.687920, learning rate 0.000010, valid_error 0.682359\n",
      "epoch 570, Train loss 0.687470, learning rate 0.000010, valid_error 0.684502\n",
      "epoch 580, Train loss 0.688923, learning rate 0.000010, valid_error 0.680810\n",
      "epoch 590, Train loss 0.688035, learning rate 0.000010, valid_error 0.681036\n",
      "epoch 600, Train loss 0.687680, learning rate 0.000005, valid_error 0.679398\n",
      "epoch 610, Train loss 0.685453, learning rate 0.000005, valid_error 0.681694\n",
      "epoch 620, Train loss 0.684036, learning rate 0.000005, valid_error 0.679941\n",
      "epoch 630, Train loss 0.684436, learning rate 0.000005, valid_error 0.679610\n",
      "epoch 640, Train loss 0.682788, learning rate 0.000005, valid_error 0.679495\n",
      "epoch 650, Train loss 0.686752, learning rate 0.000005, valid_error 0.679288\n",
      "epoch 660, Train loss 0.684505, learning rate 0.000002, valid_error 0.679668\n",
      "epoch 670, Train loss 0.682877, learning rate 0.000002, valid_error 0.679632\n",
      "epoch 680, Train loss 0.683537, learning rate 0.000002, valid_error 0.680165\n",
      "epoch 690, Train loss 0.682219, learning rate 0.000002, valid_error 0.679792\n",
      "epoch 700, Train loss 0.683274, learning rate 0.000002, valid_error 0.679467\n",
      "epoch 710, Train loss 0.682545, learning rate 0.000002, valid_error 0.679434\n",
      "epoch 720, Train loss 0.681535, learning rate 0.000001, valid_error 0.679406\n",
      "epoch 730, Train loss 0.682640, learning rate 0.000001, valid_error 0.678943\n",
      "epoch 740, Train loss 0.682539, learning rate 0.000001, valid_error 0.679253\n",
      "epoch 750, Train loss 0.683725, learning rate 0.000001, valid_error 0.679066\n",
      "epoch 760, Train loss 0.682070, learning rate 0.000001, valid_error 0.678921\n",
      "epoch 770, Train loss 0.683013, learning rate 0.000001, valid_error 0.678903\n",
      "epoch 780, Train loss 0.681996, learning rate 0.000001, valid_error 0.678718\n",
      "epoch 790, Train loss 0.681175, learning rate 0.000001, valid_error 0.678911\n",
      "epoch 800, Train loss 0.682564, learning rate 0.000001, valid_error 0.678907\n",
      "epoch 810, Train loss 0.680538, learning rate 0.000001, valid_error 0.678894\n",
      "epoch 820, Train loss 0.681522, learning rate 0.000001, valid_error 0.678925\n",
      "epoch 830, Train loss 0.681371, learning rate 0.000001, valid_error 0.678812\n",
      "epoch 840, Train loss 0.683030, learning rate 0.000000, valid_error 0.678733\n",
      "epoch 850, Train loss 0.680638, learning rate 0.000000, valid_error 0.678771\n",
      "epoch 860, Train loss 0.683577, learning rate 0.000000, valid_error 0.678758\n",
      "epoch 870, Train loss 0.681791, learning rate 0.000000, valid_error 0.678756\n",
      "epoch 880, Train loss 0.681995, learning rate 0.000000, valid_error 0.678728\n",
      "epoch 890, Train loss 0.681292, learning rate 0.000000, valid_error 0.678756\n",
      "epoch 900, Train loss 0.681437, learning rate 0.000000, valid_error 0.678779\n",
      "epoch 910, Train loss 0.682919, learning rate 0.000000, valid_error 0.678772\n",
      "epoch 920, Train loss 0.680931, learning rate 0.000000, valid_error 0.678764\n",
      "epoch 930, Train loss 0.680006, learning rate 0.000000, valid_error 0.678726\n",
      "epoch 940, Train loss 0.681110, learning rate 0.000000, valid_error 0.678743\n",
      "epoch 950, Train loss 0.681297, learning rate 0.000000, valid_error 0.678740\n",
      "epoch 960, Train loss 0.680455, learning rate 0.000000, valid_error 0.678735\n",
      "epoch 970, Train loss 0.681753, learning rate 0.000000, valid_error 0.678724\n",
      "epoch 980, Train loss 0.681181, learning rate 0.000000, valid_error 0.678718\n",
      "epoch 990, Train loss 0.680275, learning rate 0.000000, valid_error 0.678711\n"
     ]
    }
   ],
   "source": [
    "net=get_net(mx.gpu())\n",
    "train(net, 1000, 50, 0.005, 60, 0.5, data_train_nd, label_train_nd, 5e-4, mx.gpu(), data_valid_nd,label_valid_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(nd.array(test_data).as_in_context(mx.gpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = nd.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([pd.Series(test_data.index), pd.DataFrame({'Cover_Type':label.asnumpy().astype(int)}).apply(lambda x:x+1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('mlp_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
