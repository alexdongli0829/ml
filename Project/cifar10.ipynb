{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the notebook, I will write a code from beginning vs cifar10\n",
    "\n",
    "https://www.kaggle.com/c/cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed package\n",
    "import sys\n",
    "sys.path.insert(0,'../../')\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from mxnet import gluon, init, autograd, nd\n",
    "from mxnet.gluon import data as gdata, nn, loss as gloss, utils as gutils\n",
    "import datetime\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First read the file, all the files are zip in the ../../kaggle_cifar10, need first unzip them\n",
    "demo = True\n",
    "\n",
    "if demo == True:\n",
    "    import zipfile\n",
    "    for f in [\"train_tiny.zip\",\"test_tiny.zip\",\"trainLabels.csv.zip\"]:\n",
    "        with zipfile.ZipFile(\"../../data/kaggle_cifar10/\"+f,'r') as z:\n",
    "                             z.extractall(\"../../data/kaggle_cifar10/\")\n",
    "#for train.7z, use the \"7z x train.7z\" to unzip in the shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we need to reorg the data\n",
    "def reorg_data(data_dir, label_file, train_dir,test_dir,input_dir,valid_ratio):\n",
    "    \n",
    "    #read label id:label in the file\n",
    "    with open(os.path.join(data_dir,label_file)) as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        #print(lines)\n",
    "        #without rstrip, there will be \\n, for example ['1', 'frog\\n']\n",
    "        tokens = [l.rstrip().split(\",\") for l in lines]\n",
    "        #print(tokens)\n",
    "        #idx_label= dict(((int(idx), label) for idx, label in tokens))\n",
    "        idx_label= dict((int(idx), label) for idx, label in tokens)\n",
    "\n",
    "        #print(idx_label)\n",
    "    labels = set(idx_label.values())\n",
    "    print(labels)\n",
    "    \n",
    "    # got the train data number\n",
    "    n_train_all = len(os.listdir(os.path.join(data_dir,train_dir)))\n",
    "    n_train = n_train_all*(1-valid_ratio)\n",
    "    assert 0< n_train < n_train_all\n",
    "    n_train_per_label = n_train // len(labels)\n",
    "    print(n_train_per_label)\n",
    "    \n",
    "    def mkdir_if_no_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "   \n",
    "    #next we need to put the files into different folders with the folder name as the label\n",
    "    #track the file number copid to the folder\n",
    "    label_count={}\n",
    "    \n",
    "    #copy file process\n",
    "    for file in os.listdir(os.path.join(data_dir,train_dir)):\n",
    "        idx = int(file.split(\".\")[0])\n",
    "        label = idx_label[idx]\n",
    "        \n",
    "        if label not in label_count or label_count[label] < n_train_per_label:\n",
    "            \n",
    "            # put the label into dict or add 1 if already exist, make sure we can the number of train samples\n",
    "            label_count[label]=label_count.get(label,0)+1\n",
    "            #mkdir for the label \n",
    "            mkdir_if_no_exist([data_dir,input_dir,\"train\",label])\n",
    "            shutil.copy(os.path.join(data_dir,train_dir,file), os.path.join(data_dir,input_dir,\"train\",label))\n",
    "        else:\n",
    "            mkdir_if_no_exist([data_dir,input_dir,\"valid\",label])\n",
    "            shutil.copy(os.path.join(data_dir,train_dir,file), os.path.join(data_dir,input_dir,\"valid\",label))\n",
    "    \n",
    "    for file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        idx = int(file.split(\".\")[0])\n",
    "        #print(idx)\n",
    "        #label = idx_label[idx]\n",
    "        #print(label)\n",
    "        mkdir_if_no_exist([data_dir,input_dir,\"test\",\"unknown\"])\n",
    "        shutil.copy(os.path.join(data_dir,test_dir,file), os.path.join(data_dir,input_dir,\"test\",\"unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird', 'automobile', 'cat', 'truck', 'horse', 'dog', 'airplane', 'ship', 'frog', 'deer'}\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "if demo:\n",
    "    # 注意：此处使用小训练集和小测试集并将批量大小相应设小。\n",
    "    # 使用 Kaggle 比赛的完整数据集时可设批量大小为较大整数。\n",
    "    train_dir, test_dir, input_dir = 'train_tiny', 'test_tiny', \"tiny_dataset\"\n",
    "else:\n",
    "    train_dir, test_dir, input_dir = 'train', 'test', \"final_dataset\"\n",
    "\n",
    "data_dir, label_file = '../../data/kaggle_cifar10', 'trainLabels.csv'\n",
    "valid_ratio = 0.1\n",
    "reorg_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image transformation, resize, crop, flip and nomorlize\n",
    "transform_train = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.Resize(40),\n",
    "    gdata.vision.transforms.RandomResizedCrop(32, scale=(0.64,1), ratio = (1.0,1.0)),\n",
    "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
    "    #for ToTensor, check this: https://pytorch.org/docs/0.2.0/_modules/torchvision/transforms.html#ToTensor\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    gdata.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                      [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# for test, just normalize\n",
    "\n",
    "transform_test = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    gdata.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                      [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.vision.datasets.ImageFolderDataset\n",
    "train_ds = gdata.vision.ImageFolderDataset(os.path.join(data_dir, input_dir, \"train\"), flag=1)\n",
    "valid_ds = gdata.vision.ImageFolderDataset(os.path.join(data_dir, input_dir, \"valid\"), flag=1)\n",
    "test_ds = gdata.vision.ImageFolderDataset(os.path.join(data_dir, input_dir, \"test\"), flag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 32, 32)\n",
      "(10, 3, 32, 32)\n",
      "(10, 3, 32, 32)\n",
      "(10, 3, 32, 32)\n",
      "(10, 3, 32, 32)\n",
      "(10, 3, 32, 32)\n",
      "(10, 3, 32, 32)\n",
      "(10, 3, 32, 32)\n",
      "(10, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "train_data = gdata.DataLoader(train_ds.transform_first(transform_train),10, shuffle=True, last_batch='keep')\n",
    "for data,label in train_data:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def the resnet-18 network\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, num_channels, use_1x1conv = False, strides =1, **kargs):\n",
    "        \n",
    "        #for the supper, need further check why need to put the Resiual and self in the parameters\n",
    "        super(Residual, self).__init__(**kargs)\n",
    "        \n",
    "        # conv2 will not use the strides which will keep the shape I think\n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=strides)\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "        \n",
    "    def hybrid_forward(self, F, X):\n",
    "        #print(\"X:\"+str(X.shape))\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        #print(\"Y:\"+str(Y.shape))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        \n",
    "        # please note, if I use \"if use_1x1conv\", I will get NameError: name 'use_1x1conv' is not defined\n",
    "        #if use_1x1conv:\n",
    "        if self.conv3:\n",
    "            X=self.conv3(X)\n",
    "        # if 1x1 conv, the channel size maybe different and then there should be broadcast for the channel level.\n",
    "        # for the broad cast, can broadcast from 1 to n, not 2 to n, for example:\n",
    "        #Check failed: l == 1 || r == 1 operands could not be broadcast together with shapes [4,6,6,6] [4,2,6,6]\n",
    "        # But it will work if [4,6,6,6] plus [4,1,6,6]<- broadcat this\n",
    "        return F.relu(Y+X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape for x and y: (1, 3, 32, 32) (1,)\n",
    "#for x, y in train_data:\n",
    "#    print(x.shape, y.shape)\n",
    "#    print(y.astype('float32'))\n",
    "#    blk = nn.HybridSequential()\n",
    "#!!!there will be error if here I put the channel to 16, because the x has 3 channel and can not be broad cast to 16\n",
    "#    blk.add(Residual(16))\n",
    "#    blk.initialize()\n",
    "    #blk.hybridize()\n",
    "#    y=blk(x)\n",
    "# 1. I got \"module 'mxnet.symbol' has no attribute 'Relu'\" error -- should be relu\n",
    "# 2. How the x (1, 3, 32, 32) can be broadcast to more channels such as (1,64,x,x)? - answer: In resnet18, the first\n",
    "# layter is conv2d with 64 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_18(number_classes):\n",
    "    net = nn.HybridSequential()\n",
    "    \n",
    "    # first layer, 64 channel with batch normal and relu activate. One strange thing is there should be maxpool, \n",
    "    #nn.MaxPool2D(pool_size=3, strides=2, padding=1), but no here\n",
    "    net.add(nn.Conv2D(64, kernel_size=3, padding=1, strides=1), nn.BatchNorm(), nn.Activation('relu'))\n",
    "    \n",
    "    def resnet_block(num_channels, resnet_size, first_block=False):\n",
    "        blk = nn.HybridSequential()\n",
    "        for f in range(resnet_size):\n",
    "            \n",
    "            #there will be 4 residula block, each has 2 residual object. The first residual object will not half\n",
    "            # the width and height, other block will half the w and h by strides 2.\n",
    "            if f == 0 and not first_block:\n",
    "                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.add(Residual(num_channels))\n",
    "        return blk\n",
    "    \n",
    "    #add 4 residual net object, double the channel number and half the W and H, the first object should not do the \n",
    "    #half W and H before in the previous one there should be a maxpool with strides 2, however, in this project, there\n",
    "    # is no max pool, so I think we should change the W and H, need to verfiy further?????? If no 1x1conv, the  Y+X,\n",
    "    # X will be broadcast\n",
    "    net.add(resnet_block(64, 2, first_block=True))\n",
    "    net.add(resnet_block(128, 2))\n",
    "    net.add(resnet_block(256, 2))\n",
    "    net.add(resnet_block(512, 2))\n",
    "    \n",
    "    #last add last dense layer last\n",
    "    net.add(nn.GlobalAvgPool2D(), nn.Dense(number_classes))\n",
    "    \n",
    "    # 1 conv + 4 * 4 + 1 last dense = 18, thats how resnet18 come from\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and initialize the net\n",
    "def get_net(ctx):\n",
    "    num_classes = 10\n",
    "    net = resnet_18(num_classes)\n",
    "    # initialize the net with context and init from mxnet\n",
    "    net.initialize(ctx=ctx, init = init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predict result which equal to y, and then mean the value in one batch\n",
    "def train_accuracy(y_hat, y):\n",
    "    #print(y_hat.shape)\n",
    "    #print(\"y.shape\"+str(y.shape))\n",
    "    # In the example, its the mean(), I am not sure why, should be sum()\n",
    "    # I was wrong, should be mean(), the train_data len is the iterate times, for example, if 90 samples and 5 batch\n",
    "    #size, the len(train_data) should be 18 instead of 90\n",
    "    return (y_hat.argmax(axis=1)==y.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch(batch, ctx):\n",
    "    features, labels = batch\n",
    "    #print(type(features))\n",
    "    # guarantee the labels and features have the same time\n",
    "    if (features.dtype != labels.dtype):\n",
    "        labels = labels.astype(features.dtype)\n",
    "    # copy all the data into ctx\n",
    "    #Splits an NDArray into len(ctx_list) slices along batch_axis and loads each slice to one context in ctx_list.\n",
    "    # please note, the split_and_load will return list of ndarray, so to use the list, use zip\n",
    "    return (gutils.split_and_load(features, ctx), gutils.split_and_load(labels,ctx), features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \n",
    "    #data_iter type: mxnet.gluon.data.dataloader.DataLoader\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx=[ctx]\n",
    "    acc = nd.array([0])\n",
    "    n = 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        \n",
    "        #features and labels are list type\n",
    "        #use zip to convert to ndarray\n",
    "        # features type<class 'list'>\n",
    "        #print(\"features type\" + str(type(features)))\n",
    "        #print(labels)\n",
    "        \n",
    "        #!!!!!!!!!features and labels are list of ndarray, need zip to iterate the value\n",
    "        for X,y in zip(features, labels):\n",
    "        # in the original example, its sum the result, however, in the train_acc, its mean(), why????? should be sum\n",
    "        #even for the train, right?\n",
    "        # Why need to copy the cpu? Maybe because the calculation on cpu will be faster?\n",
    "            # the type for X and y is <class 'mxnet.ndarray.ndarray.NDArray'>\n",
    "            acc += (y.astype('float32') == net(X).argmax(axis=1)).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "    return acc.asscalar()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the loss function\n",
    "loss=gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the train function\n",
    "# lr_decay will change the lr, lr_period decide the period to change the lr\n",
    "def train(net, train_data, valid_data, lr, wd, epochs, lr_period, lr_decay,ctx):\n",
    "    \n",
    "    # Need to check the details for sgd, momentum defined how many steps to use for the weigth change, wd weight decay\n",
    "    # for the normalization for the loss\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr, 'momentum':0.9, 'wd':wd})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    print(prev_time)\n",
    "    for epoch in range(epochs):\n",
    "        #update the learning rate periodly\n",
    "        if epoch > 0 and epoch%lr_period==0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        \n",
    "        train_l = 0\n",
    "        train_acc = 0\n",
    "        loop_number = 0\n",
    "        \n",
    "        for X, y in train_data:\n",
    "            #loop_number += 1\n",
    "            #print(\"loop number %d\"%loop_number)\n",
    "            # in one for loop, the batch_size examples will be processed\n",
    "            y = y.astype('float32').as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X.as_in_context(ctx))\n",
    "                #print(y_hat)\n",
    "                # this loss is very interesting, the net will output 10 output, and then SoftmaxCrossEntropyLoss will\n",
    "                #calculate the loss based on the y which like a index???\n",
    "                l=loss(y_hat,y)\n",
    "            l.backward()\n",
    "            # apply the gredient based on the batch_size\n",
    "            #https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.Trainer.step.html\n",
    "            trainer.step(batch_size)\n",
    "            # in one epoch, all the loss will be sum together into the train_l, will be divided by train sample number\n",
    "            \n",
    "            #print(l)\n",
    "            train_l += l.mean().asscalar()\n",
    "            \n",
    "            # sum the whole acc, will be divided by train sample number\n",
    "            train_acc += train_accuracy(y_hat,y)\n",
    "        #The divmod() method takes two numbers and returns a pair of numbers (a tuple) consisting of their \n",
    "        #quotient and remainder.\n",
    "        current_time = datetime.datetime.now()\n",
    "        h,reminder = divmod((current_time-prev_time).seconds, 3600)\n",
    "        m,s = divmod(reminder,60)\n",
    "        #time taken time: 0:1:37 if \"time: %d:%d:%d\n",
    "        #time taken time: 00:01:37 if \"time %02d:02d:02d\"\n",
    "        time_s = \"time: %02d:%02d:%02d\" % (h,m,s)\n",
    "        \n",
    "        if epoch%1 == 0:\n",
    "            if valid_data is not None:\n",
    "                valid_acc = evaluate_accuracy(valid_data, net, ctx)\n",
    "                print(\"epoch %d, time taken %s, train loss %f, train_acc %f, valid_cc %f, learning_rate %f\" \n",
    "                      %(epoch, time_s, train_l/len(train_data), train_acc/len(train_data), valid_acc, trainer.learning_rate))\n",
    "            else:\n",
    "                print(\"epoch %d, time taken %s, train loss %f, train_acc %f, learning_rate %f\" \n",
    "                      %(epoch, time_s, train_l/len(train_data),train_acc/len(train_data), trainer.learning_rate))\n",
    "        prev_time = current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the train data, with the loader, the label will be the 0,1,2 which will be used to mark the label\n",
    "batch_size = 256\n",
    "train_data = gdata.DataLoader(train_ds.transform_first(transform_train),batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = gdata.DataLoader(valid_ds.transform_first(transform_train),batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = gdata.DataLoader(test_ds.transform_first(transform_test),batch_size, shuffle=True, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-28 23:42:25.104644\n",
      "epoch 0, time taken time: 00:00:02, train loss 2.811000, train_acc 0.100000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 1, time taken time: 00:00:01, train loss 2.598645, train_acc 0.155556, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 2, time taken time: 00:00:00, train loss 2.265529, train_acc 0.144444, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 3, time taken time: 00:00:00, train loss 2.151677, train_acc 0.222222, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 4, time taken time: 00:00:00, train loss 2.038168, train_acc 0.288889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 5, time taken time: 00:00:00, train loss 2.028507, train_acc 0.277778, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 6, time taken time: 00:00:00, train loss 1.979931, train_acc 0.255556, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 7, time taken time: 00:00:00, train loss 1.885706, train_acc 0.333333, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 8, time taken time: 00:00:00, train loss 1.761162, train_acc 0.366667, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 9, time taken time: 00:00:00, train loss 1.651979, train_acc 0.444444, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 10, time taken time: 00:00:00, train loss 1.504192, train_acc 0.488889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 11, time taken time: 00:00:00, train loss 1.450521, train_acc 0.533333, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 12, time taken time: 00:00:00, train loss 1.332956, train_acc 0.588889, valid_cc 0.000000, learning_rate 0.010000\n",
      "epoch 13, time taken time: 00:00:00, train loss 1.266109, train_acc 0.622222, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 14, time taken time: 00:00:00, train loss 1.155093, train_acc 0.633333, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 15, time taken time: 00:00:00, train loss 1.120260, train_acc 0.644444, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 16, time taken time: 00:00:00, train loss 1.054541, train_acc 0.666667, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 17, time taken time: 00:00:00, train loss 0.961467, train_acc 0.688889, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 18, time taken time: 00:00:00, train loss 0.932928, train_acc 0.722222, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 19, time taken time: 00:00:00, train loss 0.807766, train_acc 0.788889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 20, time taken time: 00:00:00, train loss 0.732310, train_acc 0.855556, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 21, time taken time: 00:00:00, train loss 0.690918, train_acc 0.822222, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 22, time taken time: 00:00:00, train loss 0.650960, train_acc 0.888889, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 23, time taken time: 00:00:00, train loss 0.558720, train_acc 0.900000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 24, time taken time: 00:00:00, train loss 0.542151, train_acc 0.900000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 25, time taken time: 00:00:00, train loss 0.469802, train_acc 0.933333, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 26, time taken time: 00:00:00, train loss 0.447711, train_acc 0.933333, valid_cc 0.000000, learning_rate 0.010000\n",
      "epoch 27, time taken time: 00:00:00, train loss 0.395074, train_acc 0.944444, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 28, time taken time: 00:00:00, train loss 0.351296, train_acc 0.955556, valid_cc 0.000000, learning_rate 0.010000\n",
      "epoch 29, time taken time: 00:00:00, train loss 0.337103, train_acc 0.955556, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 30, time taken time: 00:00:00, train loss 0.325132, train_acc 0.977778, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 31, time taken time: 00:00:00, train loss 0.274768, train_acc 0.955556, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 32, time taken time: 00:00:00, train loss 0.210530, train_acc 0.988889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 33, time taken time: 00:00:00, train loss 0.199618, train_acc 0.977778, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 34, time taken time: 00:00:00, train loss 0.203587, train_acc 0.966667, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 35, time taken time: 00:00:00, train loss 0.176787, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 36, time taken time: 00:00:00, train loss 0.131966, train_acc 0.988889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 37, time taken time: 00:00:00, train loss 0.120756, train_acc 0.988889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 38, time taken time: 00:00:00, train loss 0.118195, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 39, time taken time: 00:00:00, train loss 0.097943, train_acc 0.988889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 40, time taken time: 00:00:00, train loss 0.130021, train_acc 0.966667, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 41, time taken time: 00:00:00, train loss 0.088546, train_acc 0.988889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 42, time taken time: 00:00:00, train loss 0.092537, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 43, time taken time: 00:00:00, train loss 0.074983, train_acc 0.988889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 44, time taken time: 00:00:00, train loss 0.070766, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 45, time taken time: 00:00:00, train loss 0.063820, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 46, time taken time: 00:00:00, train loss 0.077304, train_acc 0.988889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 47, time taken time: 00:00:00, train loss 0.061051, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 48, time taken time: 00:00:00, train loss 0.074324, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 49, time taken time: 00:00:00, train loss 0.055474, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 50, time taken time: 00:00:00, train loss 0.038965, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 51, time taken time: 00:00:00, train loss 0.051818, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 52, time taken time: 00:00:00, train loss 0.039796, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 53, time taken time: 00:00:00, train loss 0.046609, train_acc 1.000000, valid_cc 0.300000, learning_rate 0.010000\n",
      "epoch 54, time taken time: 00:00:00, train loss 0.036267, train_acc 0.988889, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 55, time taken time: 00:00:00, train loss 0.051368, train_acc 0.977778, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 56, time taken time: 00:00:00, train loss 0.024304, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 57, time taken time: 00:00:00, train loss 0.028531, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 58, time taken time: 00:00:00, train loss 0.038649, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 59, time taken time: 00:00:00, train loss 0.038090, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 60, time taken time: 00:00:00, train loss 0.033753, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 61, time taken time: 00:00:00, train loss 0.020547, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 62, time taken time: 00:00:00, train loss 0.016187, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 63, time taken time: 00:00:00, train loss 0.026906, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 64, time taken time: 00:00:00, train loss 0.036720, train_acc 0.988889, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 65, time taken time: 00:00:00, train loss 0.020326, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 66, time taken time: 00:00:00, train loss 0.016361, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 67, time taken time: 00:00:00, train loss 0.028780, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 68, time taken time: 00:00:00, train loss 0.019510, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69, time taken time: 00:00:00, train loss 0.016444, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 70, time taken time: 00:00:00, train loss 0.035818, train_acc 0.988889, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 71, time taken time: 00:00:00, train loss 0.013187, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 72, time taken time: 00:00:00, train loss 0.012391, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 73, time taken time: 00:00:00, train loss 0.023558, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 74, time taken time: 00:00:00, train loss 0.014689, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 75, time taken time: 00:00:00, train loss 0.012212, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 76, time taken time: 00:00:00, train loss 0.019275, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 77, time taken time: 00:00:00, train loss 0.010556, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 78, time taken time: 00:00:00, train loss 0.010687, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.010000\n",
      "epoch 79, time taken time: 00:00:00, train loss 0.009912, train_acc 1.000000, valid_cc 0.200000, learning_rate 0.010000\n",
      "epoch 80, time taken time: 00:00:00, train loss 0.010814, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.001000\n",
      "epoch 81, time taken time: 00:00:00, train loss 0.013832, train_acc 1.000000, valid_cc 0.100000, learning_rate 0.001000\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = try_gpu(), 100, 0.01, 5e-4, \n",
    "lr_period, lr_decay, net = 80, 0.1, get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data,  lr, wd, num_epochs, lr_period,\n",
    "      lr_decay, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the parameter\n",
    "filename = os.path.join(\"../../data/\", \"resnet18.params\")\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the parameters and continue the training.\n",
    "net2 = get_net(mx.gpu())\n",
    "net2.load_parameters(filename, ctx=mx.gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx, num_epochs, lr, wd = try_gpu(), 100, 0.01, 5e-4, \n",
    "lr_period, lr_decay = 80, 0.1\n",
    "train(net2, train_data, valid_data,  lr, wd, num_epochs, lr_period,\n",
    "      lr_decay, ctx)\n",
    "\n",
    "# the continue training working, \n",
    "\n",
    "#net last output:\n",
    "\n",
    "#epoch 14, time taken time: 0:1:36, train loss 0.272228, train_acc 0.905361, valid_cc 0.818800, learning_rate 0.010000\n",
    "#epoch 15, time taken time: 0:1:36, train loss 0.259643, train_acc 0.908694, valid_cc 0.804000, learning_rate 0.010000\n",
    "        \n",
    "#net2 first ouptu:\n",
    "#epoch 0, time taken time: 0:1:35, train loss 0.227526, train_acc 0.920060, valid_cc 0.839400, learning_rate 0.010000\n",
    "#epoch 1, time taken time: 0:1:38, train loss 0.212250, train_acc 0.925165, valid_cc 0.845400, learning_rate 0.010000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary will not work for hybridized model, there will be error:\n",
    "# |      The network must have been initialized, and must not have been hybridized.\n",
    "#net2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the parameter\n",
    "filename = os.path.join(\"../../data/\", \"resnet18_2019_09_01_10_14.params\")\n",
    "net2.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
